{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers[torch]\n",
        "!pip install tensorflow -U"
      ],
      "metadata": {
        "id": "H_93RQeoDVsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGXmGWYGekEY",
        "outputId": "35c7a591-d58c-4f53-9a74-f30fa95e0d15"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFzwOHkqC7tQ",
        "outputId": "d7dccbc0-1691-43a5-879a-0fc04e6b5a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorWithPadding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "# Model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# model to device\n",
        "\n",
        "dataset = load_dataset(\"imdb\")\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=True, truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqT9vZQiC7tS"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset[\"train\"].select(range(25)).map(tokenize, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        " \"tests\": {\n",
        "  \"defaults\": {\n",
        "   \"min_pass_rate\": 1.0\n",
        "  },\n",
        "  \"robustness\": {\n",
        "   \"add_typo\": {\"min_pass_rate\": 0.7},\n",
        "   \"uppercase\": {\"min_pass_rate\": 0.7},\n",
        "   \"american_to_british\": {\"min_pass_rate\": 0.7},\n",
        "  },\n",
        "  \"accuracy\": {\n",
        "   \"min_micro_f1_score\": {\n",
        "    \"min_score\": 0.7\n",
        "   }\n",
        "  },\n",
        "  \"bias\": {\n",
        "   \"replace_to_female_pronouns\": {\n",
        "    \"min_pass_rate\": 0.7\n",
        "   },\n",
        "   \"replace_to_low_income_country\": {\n",
        "    \"min_pass_rate\": 0.7\n",
        "   }\n",
        "  }\n",
        " }\n",
        "}"
      ],
      "metadata": {
        "id": "325jnkfxfCPF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_callback = LangTestCallback(task=\"text-classification\", data={\"data_source\":\"sample.csv\"}, config=config, save_reports=True, run_each_epoch=True)"
      ],
      "metadata": {
        "id": "S6uYJ9EnFk33"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ID52Si2C7tT",
        "outputId": "ebb08896-b6a5-4d80-ef1f-9c0bdaec7730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Configuration : \n",
            " {\n",
            " \"tests\": {\n",
            "  \"defaults\": {\n",
            "   \"min_pass_rate\": 1.0\n",
            "  },\n",
            "  \"robustness\": {\n",
            "   \"add_typo\": {\n",
            "    \"min_pass_rate\": 0.7\n",
            "   },\n",
            "   \"uppercase\": {\n",
            "    \"min_pass_rate\": 0.7\n",
            "   },\n",
            "   \"american_to_british\": {\n",
            "    \"min_pass_rate\": 0.7\n",
            "   }\n",
            "  },\n",
            "  \"accuracy\": {\n",
            "   \"min_micro_f1_score\": {\n",
            "    \"min_score\": 0.7\n",
            "   }\n",
            "  },\n",
            "  \"bias\": {\n",
            "   \"replace_to_female_pronouns\": {\n",
            "    \"min_pass_rate\": 0.7\n",
            "   },\n",
            "   \"replace_to_low_income_country\": {\n",
            "    \"min_pass_rate\": 0.7\n",
            "   }\n",
            "  }\n",
            " }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    callbacks=[my_callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PzAaW4CPC7tV",
        "outputId": "f11d85ee-36f2-4341-a761-1d305391790c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2649: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  warnings.warn(\n",
            "Generating testcases...: 100%|██████████| 3/3 [00:00<00:00, 11533.37it/s]\n",
            "WARNING:root:[W009] Removing samples where no transformation has been applied:\n",
            "[W010] - Test 'add_typo': 2 samples removed out of 18\n",
            "[W010] - Test 'american_to_british': 9 samples removed out of 18\n",
            "\n",
            "WARNING:root:[W009] Removing samples where no transformation has been applied:\n",
            "[W010] - Test 'replace_to_female_pronouns': 2 samples removed out of 18\n",
            "[W010] - Test 'replace_to_low_income_country': 14 samples removed out of 18\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12/12 01:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running testcases... : 100%|██████████| 64/64 [00:17<00:00,  3.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     category                      test_type  fail_count  pass_count  \\\n",
            "0  robustness                       add_typo           0          16   \n",
            "1  robustness                      uppercase           0          18   \n",
            "2  robustness            american_to_british           0           9   \n",
            "3    accuracy             min_micro_f1_score           1           0   \n",
            "4        bias     replace_to_female_pronouns           0          16   \n",
            "5        bias  replace_to_low_income_country           0           4   \n",
            "\n",
            "  pass_rate minimum_pass_rate   pass  \n",
            "0      100%               70%   True  \n",
            "1      100%               70%   True  \n",
            "2      100%               70%   True  \n",
            "3        0%              100%  False  \n",
            "4      100%               70%   True  \n",
            "5      100%               70%   True  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning testcases... :   0%|          | 0/64 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2649: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  warnings.warn(\n",
            "Running testcases... : 100%|██████████| 64/64 [00:02<00:00, 31.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     category                      test_type  fail_count  pass_count  \\\n",
            "0  robustness                       add_typo           0          16   \n",
            "1  robustness                      uppercase           0          18   \n",
            "2  robustness            american_to_british           0           9   \n",
            "3    accuracy             min_micro_f1_score           1           0   \n",
            "4        bias     replace_to_female_pronouns           0          16   \n",
            "5        bias  replace_to_low_income_country           0           4   \n",
            "\n",
            "  pass_rate minimum_pass_rate   pass  \n",
            "0      100%               70%   True  \n",
            "1      100%               70%   True  \n",
            "2      100%               70%   True  \n",
            "3        0%              100%  False  \n",
            "4      100%               70%   True  \n",
            "5      100%               70%   True  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning testcases... :   0%|          | 0/64 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2649: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  warnings.warn(\n",
            "Running testcases... : 100%|██████████| 64/64 [00:02<00:00, 31.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     category                      test_type  fail_count  pass_count  \\\n",
            "0  robustness                       add_typo           0          16   \n",
            "1  robustness                      uppercase           0          18   \n",
            "2  robustness            american_to_british           0           9   \n",
            "3    accuracy             min_micro_f1_score           1           0   \n",
            "4        bias     replace_to_female_pronouns           0          16   \n",
            "5        bias  replace_to_low_income_country           0           4   \n",
            "\n",
            "  pass_rate minimum_pass_rate   pass  \n",
            "0      100%               70%   True  \n",
            "1      100%               70%   True  \n",
            "2      100%               70%   True  \n",
            "3        0%              100%  False  \n",
            "4      100%               70%   True  \n",
            "5      100%               70%   True  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=12, training_loss=0.0014002219152947266, metrics={'train_runtime': 67.7653, 'train_samples_per_second': 1.107, 'train_steps_per_second': 0.177, 'total_flos': 19733329152000.0, 'train_loss': 0.0014002219152947266, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Training the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langtest"
      ],
      "metadata": {
        "id": "fM7jUlPZEXJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import TrainerCallback, pipeline\n",
        "from langtest import Harness\n",
        "\n",
        "\n",
        "class LangTestCallback(TrainerCallback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        task,\n",
        "        config=None,\n",
        "        data=None,\n",
        "        print_reports=True,\n",
        "        save_reports=False,\n",
        "        run_each_epoch=False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.task = task\n",
        "        self.config = config\n",
        "        self.data = data\n",
        "        self.print_reports = print_reports\n",
        "        self.save_reports = save_reports\n",
        "        self.run_each_epoch = run_each_epoch\n",
        "        self.epoch_number = 0\n",
        "        self.harness = None\n",
        "\n",
        "    def on_init_end(self, args, state, control, **kwargs):\n",
        "        model = kwargs[\"model\"]\n",
        "        tokenizer = kwargs[\"tokenizer\"]\n",
        "\n",
        "        if self.task == \"ner\":\n",
        "            model = pipeline(\"ner\", model=model, tokenizer=tokenizer, ignore_labels=[])\n",
        "        elif self.task == \"text-classification\":\n",
        "            model = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "        self.harness = Harness(\n",
        "            self.task,\n",
        "            {\n",
        "                \"model\": model,\n",
        "                \"hub\": \"huggingface\",\n",
        "            },\n",
        "            self.data,\n",
        "            self.config,\n",
        "        )\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        self.epoch_number += 1\n",
        "\n",
        "        if self.run_each_epoch:\n",
        "            self.harness._generated_results = None\n",
        "            self.harness.run()\n",
        "            if self.print_reports:\n",
        "                print(self.harness.report())\n",
        "\n",
        "            if self.save_reports:\n",
        "                if not os.path.exists(\"reports\"):\n",
        "                    os.mkdir(\"reports\")\n",
        "                self.harness.report(\n",
        "                    format=\"markdown\",\n",
        "                    save_dir=f\"reports/report{self.epoch_number:03d}.md\",\n",
        "                )\n",
        "\n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        self.harness.generate()\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        if self.run_each_epoch:\n",
        "            return\n",
        "\n",
        "        print(self.harness.run().report())\n",
        "        if self.save_reports:\n",
        "            if not os.path.exists(\"reports\"):\n",
        "                os.mkdir(\"reports\")\n",
        "            self.harness.report(\n",
        "                format=\"markdown\", save_dir=f\"reports/report{self.epoch_number:03d}.md\"\n",
        "            )\n"
      ],
      "metadata": {
        "id": "jpUKIaftEUCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W7JT8L7TEs4e",
        "outputId": "acbfdd6f-2523-42a5-90e2-5e9570768847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow)\n",
            "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow)\n",
            "  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.0\n",
            "    Uninstalling tensorboard-2.12.0:\n",
            "      Successfully uninstalled tensorboard-2.12.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed google-auth-oauthlib-1.1.0 keras-2.15.0 tensorboard-2.15.1 tensorflow-2.15.0 tensorflow-estimator-2.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}