
<div class="h3-box" markdown="1">

## Prompt Injection Attack

 It is used for testing prompt injection vulnerabilities in LLMs. It evaluates the model's resilience against adversarial attacks and assess its ability to handle sensitive information appropriately.


**alias_name:** `prompt_injection_attack`

</div><div class="h3-box" markdown="1">

#### Config
```yaml
prompt_injection_attack:
    min_pass_rate: 0.7
```
- **min_pass_rate (float):** Minimum pass rate to pass the test.

</div><div class="h3-box" markdown="1">


</div>
