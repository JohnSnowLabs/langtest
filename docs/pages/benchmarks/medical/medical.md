---
layout: docs
header: true
seotitle: Medical Benchmark Datasets | LangTest | John Snow Labs
title: Medical Benchmark Datasets
key: benchmarks-medical
permalink: /docs/pages/benchmarks/medical/
aside:
    toc: true
sidebar:
    nav: benchmarks
show_edit_on_github: true
nav_key: benchmarks
modify_date: "2019-05-16"
---


<div class="main-docs" markdown="1">

LangTest provides support for a variety of benchmark datasets in the medical field which are listed below in the table, allowing you to assess the performance of your models on medical queries.

</div><div class="h3-box" markdown="1">


{:.table2}
| Dataset                                   | Task               | Category | Source                                                                                                                                                 | Colab                                                                                                                                                                                                                                      |
| ----------------------------------------- | ------------------ | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [**MedMCQA**](medmcqa)              | question-answering | `robustness`, `accuracy`, `fairness`        | [MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering](https://proceedings.mlr.press/v174/pal22a)  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]()             |
| [**MedQA**](medqa)              | question-answering | `robustness`, `accuracy`, `fairness`        | [What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams](https://paperswithcode.com/dataset/medqa-usmle) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]()             |
| [**PubMedQA**](pubmedqa)              | question-answering | `robustness`, `accuracy`, `fairness`        | [PubMedQA: A Dataset for Biomedical Research Question Answering](https://arxiv.org/abs/1909.06146)| [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]("")             |
| [**LiveQA**](liveqa)              | question-answering | `robustness`        | [Overview of the Medical Question Answering Task at TREC 2017 LiveQA](https://trec.nist.gov/pubs/trec26/papers/Overview-QA.pdf)| [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/Medical_Datasets.ipynb)             | 
| [**MedicationQA**](medicationqa)              | question-answering | `robustness`        | [Bridging the Gap Between Consumers' Medication Questions and Trusted Answers](https://pubmed.ncbi.nlm.nih.gov/31437878/)| [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/Medical_Datasets.ipynb)             | 
| [**HealthSearchQA**](healthsearchqa)              | question-answering | `robustness`        | [Large Language Models Encode Clinical Knowledge](https://paperswithcode.com/paper/large-language-models-encode-clinical)| [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/Medical_Datasets.ipynb)             | 

</div>