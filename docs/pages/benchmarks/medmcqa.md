---
layout: docs
header: true
seotitle: MedMCQA Benchmark | LangTest | John Snow Labs
title: MedMCQA
key: benchmarks-medmcqa
permalink: /docs/pages/benchmarks/MedMCQA/
aside:
    toc: true
sidebar:
    nav: benchmarks
show_edit_on_github: true
nav_key: benchmarks
modify_date: "2019-05-16"
---



**Source:** [MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering](https://proceedings.mlr.press/v174/pal22a)

The MedMCQA is a large-scale benchmark dataset of Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. 


{:.table2}
| subsets       | Details                                                                                                                                                                                                           |
|-------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **MedMCQA-Test**    | This dataset does not contain labels, so accuracy and fairness tests cannot be run on it. Only robustness tests can be applied.                             |
| **MedMCQA-Validation** | This dataset does contain labels, enabling the execution of robustness, accuracy, and fairness tests. |


Both the subset contains the following splits:

- Anaesthesia
- Anatomy
- Biochemistry
- Dental
- ENT
- Forensic_Medicine
- Gynaecology_Obstetrics
- Medicine
- Microbiology
- Ophthalmology
- Pathology
- Pediatrics
- Pharmacology
- Physiology
- Psychiatry
- Radiology
- Skin
- Social_Preventive_Medicine
- Surgery
- Unknown

## Benchmarks

![MedMCQA Benchmark](/assets/images/benchmarks/medmcq.png)
