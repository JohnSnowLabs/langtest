---
layout: docs
header: true
seotitle: Other Benchmarks | LangTest | John Snow Labs
title: Other Benchmarks Dataset
key: benchmarks-other_benchmarks
permalink: /docs/pages/benchmarks/other_benchmarks/
aside:
    toc: true
sidebar:
    nav: benchmarks
show_edit_on_github: true
nav_key: benchmarks
modify_date: "2019-05-16"
---


<div class="main-docs" markdown="1">
<div class="h3-box" markdown="1">


LangTest supports additional benchmark datasets for testing your models, and the listings for these datasets can be found below.

</div><div class="h3-box" markdown="1">


{:.table2}
| Dataset                                   | Task               | Category | Source                                                                                                                                                 | Colab                                                                                                                                                                                                                                      |
| ----------------------------------------- | ------------------ | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [**ASDiv**](asdiv)                        | question-answering | `robustness`, `accuracy`, `fairness`        | [A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers](https://arxiv.org/abs/2106.15772)                                   | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/ASDiv_dataset.ipynb)                  |
| [**BBQ**](bbq)                            | question-answering | `robustness`, `accuracy`, `fairness`        | [BBQ Dataset: A Hand-Built Bias Benchmark for Question Answering](https://arxiv.org/abs/2110.08193)                                                    | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/BBQ_dataset.ipynb)                    |
| [**Bigbench**](bigbench)                  | question-answering | `robustness`, `accuracy`, `fairness`        | [Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models](https://arxiv.org/abs/2206.04615)                                                                                                   | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/Bigbench_dataset.ipynb)               |
| [**BoolQ**](boolq)                        | question-answering | `robustness`, `accuracy`, `fairness`,`bias`        | [BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions](https://aclanthology.org/N19-1300/)                                           | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/BoolQ_dataset.ipynb)                  |
| [**LogiQA**](logiqa)                      | question-answering | `robustness`, `accuracy`, `fairness`        | [LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning](https://paperswithcode.com/paper/logiqa-a-challenge-dataset-for-machine)                                                                                            | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/LogiQA_dataset.ipynb)                 |
| [**MMLU**](mmlu)                          | question-answering | `robustness`, `accuracy`, `fairness`        | [MMLU: Measuring Massive Multitask Language Understanding](https://arxiv.org/abs/2009.03300)                                                           | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/mmlu_dataset.ipynb)                   |
| [**NarrativeQA**](narrativeqa)            | question-answering | `robustness`, `accuracy`, `fairness`        | [The NarrativeQA Reading Comprehension Challenge](https://aclanthology.org/Q18-1023/)                                                                  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/NarrativeQA_Question_Answering.ipynb) |
| [**NQ-open**](nq-open) | question-answering | `robustness`, `accuracy`, `fairness`        | [Natural Questions: A Benchmark for Question Answering Research](https://aclanthology.org/Q19-1026/)                                                   | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/NQ_open_dataset.ipynb)                |
| [**Quac**](quac)                          | question-answering | `robustness`, `accuracy`, `fairness`        | [Quac: Question Answering in Context](https://aclanthology.org/D18-1241/)                                                                              | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/quac_dataset.ipynb)                   |
| [**TruthfulQA**](truthfulqa)              | question-answering | `robustness`, `accuracy`, `fairness`        | [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://aclanthology.org/2022.acl-long.229/)                                                 | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/TruthfulQA_dataset.ipynb)             |
| [**XSum**](xsum)                          | summarization      | `robustness`, `accuracy`, `fairness`, `bias`        | [Donâ€™t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization](https://aclanthology.org/D18-1206/) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/XSum_dataset.ipynb)                   |


</div>