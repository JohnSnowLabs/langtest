---
layout: docs
header: true
seotitle: Benchmarks | LangTest | John Snow Labs
title: Available Benchmarks
key: notebooks
permalink: /docs/pages/benchmarks/commonsense_scenario/
aside:
    toc: true
sidebar:
    nav: benchmarks
show_edit_on_github: true
nav_key: benchmarks
modify_date: "2019-05-16"
---


<div class="main-docs" markdown="1">
<div class="h3-box" markdown="1">

LangTest offers support for diverse benchmark datasets related to commonsense scenarios, and the listings for these datasets can be found below.


</div>
</div>


{:.table2}
| Dataset                                   | Task               | Category | Source                                                                                                                                                 | Colab                                                                                                                                                                                                                                      |
| ----------------------------------------- | ------------------ | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [**CommonsenseQA**](commonsenseqa)        | question-answering | `robustness`, `accuracy`, `fairness`        | [CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge](https://arxiv.org/abs/1811.00937)                                      | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/CommonsenseQA_dataset.ipynb)          |
| [**HellaSwag**](hellaswag)                | question-answering | `robustness`, `accuracy`, `fairness`        | [HellaSwag: Can a Machine Really Finish Your Sentence?](https://aclanthology.org/P19-1472/)                                                            | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/HellaSwag_Question_Answering.ipynb)   |
| [**OpenBookQA**](openbookqa)              | question-answering | `robustness`, `accuracy`, `fairness`        | [Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering](https://arxiv.org/abs/1809.02789)                                                                                            | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/OpenbookQA_dataset.ipynb)             |
| [**PIQA**](piqa)                          | question-answering | `robustness`        | [PIQA: Reasoning about Physical Commonsense in Natural Language](https://arxiv.org/abs/1911.11641)                                                     | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/PIQA_dataset.ipynb)                   |
| [**SIQA**](siqa)                          | question-answering | `robustness`, `accuracy`, `fairness`        | [SocialIQA: Commonsense Reasoning about Social Interactions](https://arxiv.org/abs/1904.09728)                                                         | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/SIQA_dataset.ipynb)                   |
