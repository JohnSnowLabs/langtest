---
layout: docs
header: true
seotitle: SocialIQA Benchmark | LangTest | John Snow Labs
title: SocialIQA
key: benchmarks-socialiqa
permalink: /docs/pages/benchmarks/siqa/
aside:
    toc: true
sidebar:
    nav: benchmarks
show_edit_on_github: true
nav_key: benchmarks
modify_date: "2019-05-16"
---

Source: [SocialIQA: Commonsense Reasoning about Social Interactions](https://arxiv.org/abs/1904.09728)

SocialIQA is a dataset for testing the social commonsense reasoning of language models. It consists of over 1900 multiple-choice questions about various social situations and their possible outcomes or implications. The questions are based on real-world prompts from online platforms, and the answer candidates are either human-curated or machine-generated and filtered. The dataset challenges the models to understand the emotions, intentions, and social norms of human interactions.

You can see which subsets and splits are available below.

{:.table2}
| Split              | Details                                                                                |
| ------------------ | -------------------------------------------------------------------------------------- |
| **test**      | Testing set from the SIQA dataset, containing 1954 question and answer examples.       |
| **test-tiny** | Truncated version of SIQA-test dataset which contains 50 question and answer examples. |

#### Example

In the evaluation process, we start by fetching *original_context* and *original_question* from the dataset. The model then generates an *expected_result* based on this input. To assess model robustness, we introduce perturbations to the *original_context* and *original_question*, resulting in *perturbed_context* and *perturbed_question*. The model processes these perturbed inputs, producing an *actual_result*. The comparison between the *expected_result* and *actual_result* is conducted using the **QAEvalChain** approach from the LangChain library. Alternatively, users can employ metrics like **String Distance** or **Embedding Distance** to evaluate the model's performance in the Question-Answering Task within the robustness category.For a more in-depth exploration of these approaches, you can refer to this [notebook](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/misc/Evaluation_Metrics.ipynb) discussing these three methods.


{:.table3}
| category   | test_type    | original_context                                         | original_question                  | perturbed_context                                           | perturbed_question                     | expected_result                | actual_result                  | pass   |
|-----------|-------------|---------------------------------------------------------|-----------------------------------|------------------------------------------------------------|---------------------------------------|-------------------------------|-------------------------------|-------|
| robustness | dyslexia_word_swap | Tracy didn't go home that evening and resisted Riley's attacks. | What does Tracy need to do before this?<br>A. make a new plan<br>B. Go home and see Riley<br>C. Find somewhere to go | Tracy didn't go home that evening and resisted Riley's attacks. |What does Tracy need too do before this?<br>A. make a new plan<br>B. Go home and sea Riley<br>C. Find somewhere too go | C. Find somewhere to go | A. Make a new plan	  | False |


> Generated Results for `text-davinci-003` model from `OpenAI`
