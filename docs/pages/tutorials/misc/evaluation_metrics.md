---
layout: docs
header: true
seotitle: Tutorials | LangTest | John Snow Labs
title: Evaluation Metrics
key: tutorials-evaluation-metrics
permalink: /docs/pages/tutorials/misc/evaluation-metrics
sidebar:
    nav: tutorials
aside:
    toc: true
nav_key: tutorials
show_edit_on_github: true
modify_date: "2023-11-15"
---

<div class="main-docs" markdown="1"><div class="h3-box" markdown="1">

## Overview

The Evaluation Metrics notebook demonstrates the configurability of the LangTest library, especially metrics and models used for embeddings and distances. This configuration defines a system for comparing embeddings generated by different models using specified distance metrics. Two default models are provided, one from the OpenAI Hub ("text-embedding-ada-002") and another from the HuggingFace Hub ("sentence-transformers/all-mpnet-base-v2"). The distance metrics available for comparing embeddings include cosine similarity, Euclidean distance, Manhattan distance, Chebyshev distance, and Hamming distance, each with default thresholds. Users can customize the configuration by adjusting thresholds based on their specific needs. The configuration file also includes parameters for the embedding models, evaluation metrics, and test scenarios, allowing users to define various test cases and set minimum pass rates for each scenario, ensuring the robustness and effectiveness of the embedding models in different situations.

## Open in Collab

{:.table2}
| Category                                                                                                                  | Hub    | Task               | Open In Colab                                                                                                                                                                                       |
| ------------------------------------------------------------------------------------------------------------------------- |
| **Evaluation Metrics** : In this section, we discussed different evatuation metrics for evauate Quetion-Answering models. | OpenAI | Question-Answering | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/misc/Evaluation_Metrics.ipynb) |

<div class="main-docs" markdown="1"><div class="h3-box" markdown="1">


## Config Used

```yml 
tests:
  defaults:
    min_pass_rate: 0.65
  robustness:
    lowercase:
      min_pass_rate: 0.66
    uppercase:
      min_pass_rate: 0.66
```
