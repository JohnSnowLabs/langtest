---
layout: docs
header: true
seotitle: Tutorials | Benchmark Notebooks | LangTest | John Snow Labs
title: Benchmark Dataset Notebooks
key: docs-test_specific_notebooks
permalink: /docs/pages/tutorials/Benchmark_Dataset_Notebook_Notebooks
sidebar:
    nav: tutorials
aside:
    toc: true
nav_key: tutorials
show_edit_on_github: true
modify_date: "2019-05-16"
---

<div class="main-docs" markdown="1"><div class="h3-box" markdown="1">
The following table provides an overview of various Benchmark Dataset notebooks. User can select from a list of benchmark datasets provided below to test their LLMs.

</div><div class="h3-box" markdown="1">

{:.table2}
| Tutorial Description                | Hub                           | Task                              | Open In Colab                                                                                                                                                                                                                                    |
| ----------------------------------- |
| [**OpenbookQA**](https://langtest.org/docs/pages/benchmarks/openbookqa/): Evaluate your model's ability to answer questions that require complex reasoning and inference based on general knowledge, similar to an "open-book" exam.                          | OpenAI                            | Question-Answering                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/OpenbookQA_dataset.ipynb)                   |
| [**Quac**](https://langtest.org/docs/pages/benchmarks/quac/) : Evaluate your model's ability to answer questions given a conversational context, focusing on dialogue-based question-answering.                                | OpenAI                            | Question-Answering                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/quac_dataset.ipynb)                         |
| [**MMLU**](https://langtest.org/docs/pages/benchmarks/mmlu/) : Evaluate language understanding models' performance in different domains. It covers 57 subjects across STEM, the humanities, the social sciences, and more.                                | OpenAI                            | Question-Answering                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/mmlu_dataset.ipynb)                         |
| [**TruthfulQA**](https://langtest.org/docs/pages/benchmarks/truthfulqa/): Evaluate the model's capability to answer questions accurately and truthfully based on the provided information.                          | OpenAI                            | Question-Answering                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/TruthfulQA_dataset.ipynb)                   |
| [**NarrativeQA**](https://langtest.org/docs/pages/benchmarks/narrativeqa/): Evaluate your model's ability to comprehend and answer questions about long and complex narratives, such as stories or articles.                         | OpenAI                            | Question-Answering                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/NarrativeQA_Question_Answering.ipynb)       |
| [**HellaSWag**](https://langtest.org/docs/pages/benchmarks/hellaswag/): Evaluate your model's ability in completions of sentences.                           | OpenAI                            | Question-Answering                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/HellaSwag_Question_Answering.ipynb)         |
| [**BBQ**](https://langtest.org/docs/pages/benchmarks/bbq/): Evaluate how your model responds to questions in the presence of social biases against protected classes across various social dimensions.                                 | OpenAI                            | Question-Answering                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/BBQ_dataset.ipynb)                          |
| [**NQ open**](https://langtest.org/docs/pages/benchmarks/nq-open/): Evaluate the ability of your model to answer open-ended questions based on a given passage or context.                             | OpenAI                            | Question-Answering                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/NQ_open_dataset.ipynb)                      |
| [**BoolQ**](https://langtest.org/docs/pages/benchmarks/boolq/): Evaluate the ability of your model to answer boolean questions (yes/no) based on a given passage or context.                               | OpenAI                            | Question-Answering                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/BoolQ_dataset.ipynb)                        |
| [**XSum**](https://langtest.org/docs/pages/benchmarks/xsum/) : Evaluate your model's ability to generate concise and informative summaries for long articles with the XSum dataset. | OpenAI                            | Summarization                     | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/XSum_dataset.ipynb)                         |
| [**LogiQA**](https://langtest.org/docs/pages/benchmarks/logiqa/): Evaluate your model's accuracy on Machine Reading Comprehension with Logical Reasoning questions.                             | OpenAI                            | Question-Answering                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/LogiQA_dataset.ipynb)                       |
| [**ASDiv**](https://langtest.org/docs/pages/benchmarks/asdiv/): Evaluate your model's ability answer questions based on Math Word Problems.                                | OpenAI                            | Question-Answering                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/ASDiv_dataset.ipynb)                        |
| [**BigBench**](https://langtest.org/docs/pages/benchmarks/bigbench/): Evaluate your model's performance on BigBench datasets by Google.                            | OpenAI                            | Question-Answering                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/Bigbench_dataset.ipynb)                     |
| [**MultiLexSum**](https://langtest.org/docs/pages/benchmarks/multilexsum/): Evaluate your model's ability to generate concise and informative summaries for legal case contexts from the Multi-LexSum dataset                         | OpenAI                            | Summarization                     | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/MultiLexSum_dataset.ipynb)                  |
| [**Legal-QA**](https://langtest.org/docs/pages/benchmarks/privacy-policy/): Evaluate your model's performance on legal-qa datasets                            | OpenAI                            | Legal-Tests                       | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/LegalQA_Datasets.ipynb)                     |
| [**CommonSenseQA**](https://langtest.org/docs/pages/benchmarks/commonsenseqa/): Evaluate your model's performance on the CommonsenseQA dataset                       | OpenAI                            | Question-Answering                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/CommonsenseQA_dataset.ipynb)                |
| [**SIQA**](https://langtest.org/docs/pages/benchmarks/siqa/): Evaluate your model's performance by assessing its accuracy in understanding social situations.                                | OpenAI                            | Question-Answering                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/SIQA_dataset.ipynb)                         |
| [**PIQA**](https://langtest.org/docs/pages/benchmarks/piqa/): Evaluate your model's performance on the PIQA dataset, which tests its ability to reason about everyday physical situations                               | OpenAI                            | Question-Answering                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/PIQA_dataset.ipynb)                         |
| [**Fiqa**](https://langtest.org/docs/pages/benchmarks/fiqa/): Evaluate your model's performance on the FiQA dataset, a comprehensive and specialized resource designed for finance-related question-answering tasks.                         | OpenAI                     | Question-Answering                       | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/dataset-notebooks/Fiqa_dataset.ipynb)  |