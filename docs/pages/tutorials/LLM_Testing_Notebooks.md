---
layout: docs
header: true
seotitle: Tutorials | LangTest | John Snow Labs
title: LLM Testing Notebooks
key: docs-test_specific_notebooks
permalink: /docs/pages/tutorials/LLM_Testing_Notebooks
sidebar:
    nav: tutorials
aside:
    toc: true
nav_key: tutorials
show_edit_on_github: true
modify_date: "2019-05-16"
---

<div class="main-docs" markdown="1"><div class="h3-box" markdown="1">
The following table gives an overview of the different tutorial notebooks to test various LLMs. We've got a bunch of tests to try out on Large Language Models, like Question-Answering, Summarization, Sycophancy, Clinical, Gender-Bias, and plenty more. Please refer the below table for more options.

</div><div class="h3-box" markdown="1">

{:.table2}
| Tutorial Description                | Hub                           | Task                              | Open In Colab                                                                                                                                                                                                                                    |
| ----------------------------------- |
| OpenAI Model Testing For Question Answering and Summarization                 | OpenAI                            | Question-Answering/Summarization  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/OpenAI_QA_Summarization_Testing_Notebook.ipynb)               |
| Ai21 Model Testing For Question Answering and Summarization                 | AI21                              | Question-Answering/Summarization  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/AI21_QA_Summarization_Testing_Notebook.ipynb)                 |
| Cohere Model Testing For Question Answering and Summarization                 | Cohere                            | Question-Answering/Summarization  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/Cohere_QA_Summarization_Testing_Notebook.ipynb)               |
| Hugging Face Inference API Model Testing For Question Answering and Summarization                 | Hugging Face Inference API        | Question-Answering/Summarization  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/HuggingFaceAPI_QA_Summarization_Testing_Notebook.ipynb)       |
| Hugging Face Hub Model Testing For Question Answering and Summarization                 | Hugging Face Hub                  | Question-Answering/Summarization  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/HuggingFaceHub_QA_Summarization_Testing_Notebook.ipynb)       |
| Azure-OpenAI Model Testing For Question Answering and Summarization                 | Azure-OpenAI                      | Question-Answering/Summarization  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/Azure_OpenAI_QA_Summarization_Testing_Notebook.ipynb)         |
| Evaluating `text-davinci-003` model on toxicity test                       | OpenAI                            | Toxicity                          | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/Toxicity_NB.ipynb)                                            |
| Assess any demographic bias the model might exhibit when suggesting treatment plans for two patients with identical diagnoses.             | OpenAI                            | Clinical-Tests                    | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/Clinical_Tests.ipynb)                                         |
| Evaluating the model in capturing nuanced political beliefs beyond the traditional left-right spectrum.                     | OpenAI                            | Political                         | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/test-specific-notebooks/Political_Demo.ipynb)                               |
| In this tutorial, we assess the model's capability to generate disinformation.                       | AI21                              | Disinformation-Test               | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/Disinformation_Test.ipynb)                                    |
| In this tutorial, we assess how well LLMs can identify the factual accuracy of summary sentences. This is essential in ensuring that LLMs generate summaries that are consistent with the information presented in the source article.                    | OpenAI                            | Factuality                        | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/Factuality_Test.ipynb)                                        |
| In this tutorial, we assess the model on LegalSupport dataset. Each sample consists of a text passage making a legal claim, and two case summaries.                      | OpenAI                            | Legal-Tests                       | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/Legal_Support.ipynb)                                          |
| In this tutorial, we assess the prompt injection vulnerabilities in LLMs. It evaluates the modelâ€™s resilience against adversarial attacks and assess its ability to handle sensitive information appropriately.          | OpenAI                            | Security                          | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/Prompt_Injections_Tests.ipynb)                                |
| In this tutorial, we assess the model sensitivity by adding negation and toxic words to see analyze the behaviour in the LLM response.                    | OpenAI                            | Sensitivity-Test                  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/Sensitivity_Test.ipynb)                                       |
| Sycophancy is an undesirable behavior in which models tailor their responses to align with a human user's view, even when that view is not objectively correct. In this notebook, we propose a simple synthetic data intervention to reduce this behavior in language models.                      | OpenAI                            | Sycophancy                        | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/Sycophancy_test.ipynb)                                        |
| In this tutorial, we assess the model on gender occupational stereotype statements.                         | OpenAI                      | Wino-Bias                       | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/langtest/blob/main/demo/tutorials/llm_notebooks/Wino_Bias_LLM.ipynb)                         |