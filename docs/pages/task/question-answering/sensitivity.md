
<div class="h3-box" markdown="1">

## Sensitivity

The primary objective of the sensitivity test is to assess the model’s responsiveness when introducing negation and toxic words, gauging its level of sensitivity in these scenarios.

**How it works:**

- Introducing perturbations to the *original* text, resulting in perturbed *test_case*.
- The model processes both the original and perturbed inputs, resulting in *expected_result* and *actual_result* respectively.

> Evaluation Criteria in **Negation**:

![Sensitivity-Negation Generated Results](/assets/images/task/question-answering-sensitivity-negation.png)

- If the model is hosted using an Api, we proceed by calculating the embeddings of both the expected response and actual response. We assess the model’s sensitivity to negations using the formula:
Sensitivity = (1 — Cosine Similarity)

- In the case where the model is hosted under the Huggingface hub, we first retrieve both the model and the tokenizer from the hub. Next, we encode the text for both the expected response and actual response and subsequently calculate the loss between the outputs of the model.

- Threshold: A predefined threshold of (-0.2,0.2) is set as the default. If the eval_score falls within this threshold range, it indicates that the model is failing to properly handle negations, implying insensitivity to linguistic nuances introduced by negation words. You can also give the threshold value for the test as per your choice while defining the config.
 
> Evaluation Criteria in **toxicity**:

![Sensitivity-Toxicity Generated Results](/assets/images/task/question-answering-sensitivity-toxicity.png)

- Counting Bad Words: It’s important to note that we employ a list of toxic words during the evaluation process. This list helps us identify and quantify the presence of toxic words in both the expected and actual results.

- Calculating Evaluation Score: The evaluation score is calculated as we subtract the count of bad words in the expected result from the count of bad words in the actual result.
Threshold: A predefined threshold of 0 is set as the default .If the evaluation score is greater than 0, the test result is `False`, indicating that the model is sensitive to the addition of bad words. You can also give the threshold value for the test as per your choice while defining the config.

</div>